{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhSCrLdD1KNM",
        "outputId": "581f14fc-745a-4b2c-a3d0-948dce73b744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.12/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (2.32.4)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (0.5.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras->keras-tuner) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras-tuner) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras-tuner) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n"
      ],
      "metadata": {
        "id": "xSJa9qFC1nNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/funnyPhani/HouseData/main/kc_house_data.csv\"\n",
        "data = pd.read_csv(url)\n",
        "data.drop(['id', 'date'], axis=1, inplace=True)\n",
        "assert data.isnull().sum().sum() == 0, \"Data contains missing values!\""
      ],
      "metadata": {
        "id": "mxQCWO851blJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop('price', axis=1)\n",
        "y = np.log1p(data['price'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#(Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "rAta18Zk2SHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(units=hp.Int('units_input', 64, 128, step=32),\n",
        "                    activation='relu',\n",
        "                    kernel_regularizer=l2(0.001),\n",
        "                    input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    for i in range(hp.Int('num_layers', 1, 2)):  # 1–2 layers max\n",
        "        model.add(Dense(units=hp.Int(f'units_{i}', 64, 128, step=32),\n",
        "                        activation='relu',\n",
        "                        kernel_regularizer=l2(0.001)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(rate=hp.Float(f'dropout_{i}', 0.1, 0.3, step=0.1)))\n",
        "\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "fLaSIWcA2jJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter tuning using Keras Tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    directory='my_dir',\n",
        "    project_name='house_price_mlp'\n",
        ")\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "tuner.search(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stop], verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_3vnALO2mQg",
        "outputId": "b9ef8542-062e-4d46-e62d-715e3422f5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 07m 09s]\n",
            "val_loss: 33552435200.0\n",
            "\n",
            "Best val_loss So Far: 13615249408.0\n",
            "Total elapsed time: 01h 11m 00s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the best model\n",
        "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
        "print(\"Best hyperparameters:\")\n",
        "print(f\"Units input: {best_hp.get('units_input')}\")\n",
        "print(f\"Number of layers: {best_hp.get('num_layers')}\")\n",
        "for i in range(best_hp.get('num_layers')):\n",
        "    print(f\"Layer {i} units: {best_hp.get(f'units_{i}')}\")\n",
        "    print(f\"Layer {i} dropout: {best_hp.get(f'dropout_{i}')}\")\n",
        "print(f\"Learning rate: {best_hp.get('learning_rate')}\")\n",
        "\n",
        "best_model = tuner.hypermodel.build(best_hp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb4T4KIU2mfg",
        "outputId": "162f90de-e475-4691-96b1-783f180054be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters:\n",
            "Units input: 160\n",
            "Number of layers: 3\n",
            "Layer 0 units: 160\n",
            "Layer 0 dropout: 0.2\n",
            "Layer 1 units: 96\n",
            "Layer 1 dropout: 0.30000000000000004\n",
            "Layer 2 units: 256\n",
            "Layer 2 dropout: 0.2\n",
            "Learning rate: 0.002130209316625156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the best model on full training data\n",
        "history = best_model.fit(X_train, y_train, epochs=100, validation_split=0.2,\n",
        "                         callbacks=[early_stop], verbose=1)\n",
        "\n",
        "#Evaluate model on test set\n",
        "y_pred = best_model.predict(X_test).flatten()\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Test MSE: {mse:.2f}\")\n",
        "print(f\"Test MAE: {mae:.2f}\")\n",
        "print(f\"Test R^2 Score: {r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnl5-SzS2mgG",
        "outputId": "3a652ad0-2d03-49fd-810e-84742cbcd4ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 210626723840.0000 - mae: 321944.1250 - val_loss: 31080888320.0000 - val_mae: 115863.5938\n",
            "Epoch 2/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 34154233856.0000 - mae: 118853.9219 - val_loss: 28635805696.0000 - val_mae: 110389.3750\n",
            "Epoch 3/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 32827043840.0000 - mae: 115629.5312 - val_loss: 27989065728.0000 - val_mae: 108844.3203\n",
            "Epoch 4/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 33805881344.0000 - mae: 117005.1641 - val_loss: 27408590848.0000 - val_mae: 106592.7500\n",
            "Epoch 5/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 34335545344.0000 - mae: 116061.5859 - val_loss: 26608257024.0000 - val_mae: 105503.3594\n",
            "Epoch 6/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 32901781504.0000 - mae: 115023.6328 - val_loss: 29294067712.0000 - val_mae: 111674.3516\n",
            "Epoch 7/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 35109462016.0000 - mae: 116172.0234 - val_loss: 27822272512.0000 - val_mae: 109546.8516\n",
            "Epoch 8/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 34587107328.0000 - mae: 114036.9766 - val_loss: 25839241216.0000 - val_mae: 104145.9141\n",
            "Epoch 9/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 29871343616.0000 - mae: 109891.4141 - val_loss: 27275702272.0000 - val_mae: 105855.7578\n",
            "Epoch 10/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 29702944768.0000 - mae: 109154.8359 - val_loss: 25966993408.0000 - val_mae: 102196.9609\n",
            "Epoch 11/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 30082684928.0000 - mae: 109815.6484 - val_loss: 25526011904.0000 - val_mae: 102044.0469\n",
            "Epoch 12/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 31014643712.0000 - mae: 110383.6094 - val_loss: 25349865472.0000 - val_mae: 101138.0625\n",
            "Epoch 13/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 28290416640.0000 - mae: 107098.3281 - val_loss: 24185268224.0000 - val_mae: 98928.1016\n",
            "Epoch 14/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 28132882432.0000 - mae: 107034.1797 - val_loss: 24208494592.0000 - val_mae: 98048.7031\n",
            "Epoch 15/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 28176412672.0000 - mae: 105274.2500 - val_loss: 23866933248.0000 - val_mae: 97597.0078\n",
            "Epoch 16/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 27736025088.0000 - mae: 105683.0781 - val_loss: 25731452928.0000 - val_mae: 101804.5703\n",
            "Epoch 17/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 27598221312.0000 - mae: 104314.4297 - val_loss: 22813124608.0000 - val_mae: 94612.3438\n",
            "Epoch 18/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 26203273216.0000 - mae: 102628.0234 - val_loss: 21904285696.0000 - val_mae: 90716.1719\n",
            "Epoch 19/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 25729583104.0000 - mae: 102514.5234 - val_loss: 21795184640.0000 - val_mae: 90495.9922\n",
            "Epoch 20/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 23804645376.0000 - mae: 97842.3438 - val_loss: 20804997120.0000 - val_mae: 87520.5625\n",
            "Epoch 21/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 23758336000.0000 - mae: 99519.6406 - val_loss: 21812324352.0000 - val_mae: 88644.4297\n",
            "Epoch 22/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 21933678592.0000 - mae: 93533.2188 - val_loss: 19636137984.0000 - val_mae: 84577.0156\n",
            "Epoch 23/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 24712747008.0000 - mae: 97925.6797 - val_loss: 21210736640.0000 - val_mae: 86351.6016\n",
            "Epoch 24/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 23574075392.0000 - mae: 95634.2188 - val_loss: 19704621056.0000 - val_mae: 83510.8203\n",
            "Epoch 25/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 22810056704.0000 - mae: 93565.3906 - val_loss: 18227597312.0000 - val_mae: 80749.6484\n",
            "Epoch 26/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 19953223680.0000 - mae: 89483.3203 - val_loss: 22051493888.0000 - val_mae: 84841.2188\n",
            "Epoch 27/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 19818616832.0000 - mae: 89822.7188 - val_loss: 21245995008.0000 - val_mae: 89394.3125\n",
            "Epoch 28/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 20513921024.0000 - mae: 91388.2500 - val_loss: 19276488704.0000 - val_mae: 83042.2578\n",
            "Epoch 29/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 20034318336.0000 - mae: 89750.2188 - val_loss: 17539727360.0000 - val_mae: 77906.8281\n",
            "Epoch 30/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 17579771904.0000 - mae: 86236.3438 - val_loss: 16816484352.0000 - val_mae: 78425.2500\n",
            "Epoch 31/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 18691084288.0000 - mae: 87923.5000 - val_loss: 15612058624.0000 - val_mae: 73660.8594\n",
            "Epoch 32/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 18616467456.0000 - mae: 86974.0703 - val_loss: 19424671744.0000 - val_mae: 84790.4844\n",
            "Epoch 33/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 19927474176.0000 - mae: 88262.5703 - val_loss: 16755167232.0000 - val_mae: 77973.5469\n",
            "Epoch 34/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 17767206912.0000 - mae: 86958.5000 - val_loss: 17007306752.0000 - val_mae: 79922.6719\n",
            "Epoch 35/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 16901490688.0000 - mae: 84135.4922 - val_loss: 14941328384.0000 - val_mae: 73208.0156\n",
            "Epoch 36/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 18268780544.0000 - mae: 86094.5547 - val_loss: 15571582976.0000 - val_mae: 74732.3047\n",
            "Epoch 37/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 16638109696.0000 - mae: 84079.3281 - val_loss: 14952056832.0000 - val_mae: 73686.7031\n",
            "Epoch 38/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 16871434240.0000 - mae: 83322.0938 - val_loss: 14961215488.0000 - val_mae: 71813.2109\n",
            "Epoch 39/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 16338795520.0000 - mae: 82610.2109 - val_loss: 15657408512.0000 - val_mae: 72027.5547\n",
            "Epoch 40/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 16898036736.0000 - mae: 82934.3906 - val_loss: 15745679360.0000 - val_mae: 73265.0234\n",
            "Epoch 41/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 15581733888.0000 - mae: 81085.8672 - val_loss: 15447364608.0000 - val_mae: 71417.1641\n",
            "Epoch 42/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 17363535872.0000 - mae: 83652.8828 - val_loss: 15170918400.0000 - val_mae: 74089.9141\n",
            "Epoch 43/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 16330138624.0000 - mae: 83110.4297 - val_loss: 16307138560.0000 - val_mae: 77742.8516\n",
            "Epoch 44/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 16112516096.0000 - mae: 81706.6016 - val_loss: 20456050688.0000 - val_mae: 82043.7812\n",
            "Epoch 45/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 18413400064.0000 - mae: 84704.4297 - val_loss: 13648192512.0000 - val_mae: 68774.5781\n",
            "Epoch 46/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 16763102208.0000 - mae: 82388.0703 - val_loss: 14696299520.0000 - val_mae: 69556.2188\n",
            "Epoch 47/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 14972005376.0000 - mae: 80618.3594 - val_loss: 14679669760.0000 - val_mae: 71539.8203\n",
            "Epoch 48/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 16702859264.0000 - mae: 82138.2656 - val_loss: 13740735488.0000 - val_mae: 70541.3984\n",
            "Epoch 49/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 15427362816.0000 - mae: 80372.4766 - val_loss: 13810780160.0000 - val_mae: 68908.0000\n",
            "Epoch 50/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 15105846272.0000 - mae: 81611.0938 - val_loss: 18384918528.0000 - val_mae: 94087.9375\n",
            "Epoch 51/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 17083146240.0000 - mae: 83887.8359 - val_loss: 14066813952.0000 - val_mae: 68633.5000\n",
            "Epoch 52/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 15810964480.0000 - mae: 82475.0469 - val_loss: 14390781952.0000 - val_mae: 73457.5625\n",
            "Epoch 53/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 14839655424.0000 - mae: 79899.0000 - val_loss: 13348874240.0000 - val_mae: 68448.6562\n",
            "Epoch 54/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 14582381568.0000 - mae: 78457.3906 - val_loss: 13374025728.0000 - val_mae: 68467.3281\n",
            "Epoch 55/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 14348379136.0000 - mae: 79256.3906 - val_loss: 14850000896.0000 - val_mae: 69554.6953\n",
            "Epoch 56/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 15184329728.0000 - mae: 80561.8203 - val_loss: 14633897984.0000 - val_mae: 70667.0547\n",
            "Epoch 57/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 15089788928.0000 - mae: 80137.7031 - val_loss: 14737404928.0000 - val_mae: 73224.2812\n",
            "Epoch 58/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 16522685440.0000 - mae: 82530.7656 - val_loss: 13563287552.0000 - val_mae: 67969.0156\n",
            "Epoch 59/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 17251979264.0000 - mae: 82631.1328 - val_loss: 14479006720.0000 - val_mae: 71992.4297\n",
            "Epoch 60/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 13665128448.0000 - mae: 76969.6094 - val_loss: 15379628032.0000 - val_mae: 70913.6719\n",
            "Epoch 61/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 14637785088.0000 - mae: 78454.1719 - val_loss: 13612962816.0000 - val_mae: 70253.7656\n",
            "Epoch 62/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 14568036352.0000 - mae: 79495.5625 - val_loss: 14879537152.0000 - val_mae: 74578.6641\n",
            "Epoch 63/100\n",
            "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 14659220480.0000 - mae: 80052.0469 - val_loss: 15171737600.0000 - val_mae: 71168.0625\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Test MSE: 18747537617.73\n",
            "Test MAE: 77033.59\n",
            "Test R^2 Score: 0.8760\n"
          ]
        }
      ]
    }
  ]
}